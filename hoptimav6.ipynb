{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate, SpatialDropout1D, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import re\n",
    "import pickle\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for model and visualization artifacts\"\"\"\n",
    "    directories = ['saved_models', 'visualizations']\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove URLs dan email\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Handle a``ngka dengan konteks\n",
    "    text = re.sub(r'(\\d+)\\s*(milyar|miliar|m)', r'\\1000000000', text)\n",
    "    text = re.sub(r'(\\d+)\\s*(juta|jt)', r'\\1000000', text)\n",
    "    \n",
    "    # Remove special characters tapi pertahankan yang penting\n",
    "    text = re.sub(r'[^\\w\\s+\\-.,]', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Baca dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Hapus baris yang kosong\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Inisialisasi Sastrawi\n",
    "    stemmer_factory = StemmerFactory()\n",
    "    stemmer = stemmer_factory.create_stemmer()\n",
    "    stopword_factory = StopWordRemoverFactory()\n",
    "    stopword = stopword_factory.create_stop_word_remover()\n",
    "    \n",
    "    # Terapkan pembersihan teks dengan weighted concatenation yang dimodifikasi\n",
    "    df['clean_description'] = (\n",
    "        (df['Judul_Clean'].astype(str) + ' ' + df['Judul_Clean'].astype(str)) + ' ' +  # Bobot 2x\n",
    "        df['Lokasi_Clean'].astype(str) + ' ' +  # Bobot 1x\n",
    "        df['Deskripsi_Clean'].astype(str) + ' ' +  # Bobot 1x\n",
    "        (df['Keywords_Clean'].astype(str) + ' ' + df['Keywords_Clean'].astype(str))  # Bobot 2x\n",
    "    )\n",
    "    \n",
    "    # Bersihkan teks gabungan\n",
    "    df['clean_description'] = df['clean_description'].apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    # Tokenisasi teks dengan konfigurasi yang dioptimalkan\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=10000,  # Meningkatkan vocabulary size\n",
    "        oov_token='<OOV>',\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True\n",
    "    )\n",
    "    tokenizer.fit_on_texts(df['clean_description'])\n",
    "    \n",
    "    # Convert teks ke sequences dengan padding yang dioptimalkan\n",
    "    sequences = tokenizer.texts_to_sequences(df['clean_description'])\n",
    "    padded_sequences = pad_sequences(\n",
    "        sequences, \n",
    "        maxlen=300,  # Meningkatkan maksimum length\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Ensure directory exists before saving\n",
    "    create_directories()\n",
    "    \n",
    "    # Simpan tokenizer\n",
    "    with open('saved_models/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Numeric features dengan normalisasi yang ditingkatkan\n",
    "    numeric_features = df[[\n",
    "        'Harga_Normalized', 'Kamar_Normalized', 'WC_Normalized',\n",
    "        'Parkir_Normalized', 'Luas_Tanah_Normalized', 'Luas_Bangunan_Normalized'\n",
    "    ]].values\n",
    "    \n",
    "    # Tambahkan feature engineering\n",
    "    additional_features = np.column_stack([\n",
    "        numeric_features,\n",
    "        df['Harga_Normalized'] / df['Luas_Bangunan_Normalized'],  # Price per sqm\n",
    "        df['Luas_Bangunan_Normalized'] / df['Luas_Tanah_Normalized'],  # Building ratio\n",
    "        df['Kamar_Normalized'] * df['WC_Normalized']  # Room-bathroom ratio\n",
    "    ])\n",
    "    \n",
    "    return padded_sequences, additional_features, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevance_labels(df, query_features):\n",
    "    \"\"\"\n",
    "    Membuat label relevansi berdasarkan kecocokan fitur properti dengan query\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    for _, property in df.iterrows():\n",
    "        score = 0\n",
    "        total_weights = 0\n",
    "        \n",
    "        # Checking price match (weight: 0.3)\n",
    "        if property['Harga'] <= query_features['max_price']:\n",
    "            score += 0.3\n",
    "        total_weights += 0.3\n",
    "        \n",
    "        # Checking location match (weight: 0.25)\n",
    "        if query_features['location'] in property['Lokasi_Clean'].lower():\n",
    "            score += 0.25\n",
    "        total_weights += 0.25\n",
    "        \n",
    "        # Checking room requirements (weight: 0.15)\n",
    "        if property['Kamar'] >= query_features['min_bedrooms']:\n",
    "            score += 0.15\n",
    "        total_weights += 0.15\n",
    "        \n",
    "        # Checking bathroom requirements (weight: 0.15)\n",
    "        if property['WC'] >= query_features['min_bathrooms']:\n",
    "            score += 0.15\n",
    "        total_weights += 0.15\n",
    "        \n",
    "        # Checking parking requirements (weight: 0.15)\n",
    "        if property['Parkir'] >= query_features['min_parking']:\n",
    "            score += 0.15\n",
    "        total_weights += 0.15\n",
    "        \n",
    "        # Normalize score\n",
    "        final_score = score / total_weights\n",
    "        \n",
    "        # Convert to binary label with threshold\n",
    "        labels.append(1 if final_score >= 0.7 else 0)\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_queries():\n",
    "    \"\"\"\n",
    "    Membuat dataset training queries yang realistis\n",
    "    \"\"\"\n",
    "    locations = ['sleman', 'bantul', 'yogyakarta', 'jogja', 'kulon progo', 'gunung kidul']\n",
    "    price_ranges = [\n",
    "        {'min': 0.5, 'max': 1.0},\n",
    "        {'min': 1.0, 'max': 1.5},\n",
    "        {'min': 1.5, 'max': 2.0},\n",
    "        {'min': 2.0, 'max': 3.0},\n",
    "    ]\n",
    "    \n",
    "    training_queries = []\n",
    "    \n",
    "    for loc in locations:\n",
    "        for price in price_ranges:\n",
    "            for bedrooms in range(1, 6):\n",
    "                for bathrooms in range(1, 4):\n",
    "                    for parking in range(0, 4):\n",
    "                        query_features = {\n",
    "                            'location': loc,\n",
    "                            'max_price': price['max'] * 1e9,  # Convert to rupiah\n",
    "                            'min_bedrooms': bedrooms,\n",
    "                            'min_bathrooms': bathrooms,\n",
    "                            'min_parking': parking\n",
    "                        }\n",
    "                        \n",
    "                        query = f\"Cari rumah di {loc} dengan {bedrooms} kamar tidur, \"\n",
    "                        query += f\"{bathrooms} kamar mandi, parkir {parking} mobil \"\n",
    "                        query += f\"dibawah {price['max']} milyar\"\n",
    "                        \n",
    "                        training_queries.append((query, query_features))\n",
    "    \n",
    "    return training_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Model dengan arsitektur yang dioptimasi dan flow data yang benar,\n",
    "    dengan beberapa optimasi dari model optimized namun tetap mempertahankan\n",
    "    struktur inti.\n",
    "    \"\"\"\n",
    "    # Text processing branch\n",
    "    text_input = Input(shape=(300,), name='text_input')\n",
    "    # Convert text indices to embeddings - tetap menggunakan 256 sesuai original\n",
    "    embedding = Embedding(vocab_size, embedding_dim, mask_zero=False)(text_input)\n",
    "    \n",
    "    # Multiple parallel convolution layers - optimasi jumlah filter\n",
    "    conv_layers = []\n",
    "    for filter_size in [3, 4]:  # Tetap menggunakan 3 filter sesuai original\n",
    "        conv = Conv1D(32, 3, activation='relu')(embedding)  # Kurangi filter\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        conv_layers.append(pool)\n",
    "    \n",
    "    conv_concat = Concatenate()(conv_layers)\n",
    "    \n",
    "    # LSTM branch - tetap bidirectional sesuai original dengan optimasi ukuran\n",
    "    lstm = Bidirectional(LSTM(96, return_sequences=True))(embedding)  # Optimasi ke 96 dari 128\n",
    "    lstm = Bidirectional(LSTM(48))(lstm)  # Optimasi ke 48 dari 64\n",
    "    \n",
    "    # Combine text features dengan dropout yang dioptimasi\n",
    "    text_features = Concatenate()([conv_concat, lstm])\n",
    "    text_features = Dropout(0.2)(pool)\n",
    "    \n",
    "    # Numeric features branch - optimasi arsitektur\n",
    "    numeric_input = Input(shape=(9,), name='numeric_input')\n",
    "    numeric_dense = Dense(32, activation='relu')(numeric_input)\n",
    "    numeric_dense = BatchNormalization()(numeric_dense)\n",
    "    numeric_dense = Dropout(0.2)(numeric_dense)\n",
    "    numeric_dense = Dense(48, activation='relu')(numeric_dense)  # Optimasi ke 48 dari 64\n",
    "    \n",
    "    # Merge all features\n",
    "    merged = Concatenate()([text_features, numeric_dense])\n",
    "    \n",
    "    # Deep classification layers dengan optimasi\n",
    "    dense = Dense(64, activation='relu')(merged)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(64, activation='relu')(merged)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    #dense = Dense(96, activation='relu', kernel_regularizer=l2(0.005))(dense)  # Optimasi dari 128 dan l2\n",
    "    #dense = BatchNormalization()(dense)\n",
    "    #dense = Dropout(0.25)(dense)  # Optimasi dari 0.3\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    # Create model with multiple inputs\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'text_input': text_input,\n",
    "            'numeric_input': numeric_input\n",
    "        },\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedule yang dioptimasi\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1200  # Optimasi dari 1000\n",
    "    decay_rate = 0.85   # Optimasi dari 0.9\n",
    "    learning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps, decay_rate\n",
    "    )\n",
    "    \n",
    "    # Optimizer with gradient clipping\n",
    "    optimizer = Adam(\n",
    "        learning_rate=learning_rate_fn,\n",
    "        clipnorm=0.8  # Optimasi dari 1.0\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.F1Score()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_model_batched(df, padded_sequences, numeric_features, tokenizer, batch_size=256):  # Optimasi batch size\n",
    "    \"\"\"\n",
    "    Training dengan pendekatan batch processing menggunakan tf.data.Dataset\n",
    "    dengan optimasi performa\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_tf_dataset(query_data, numeric_data, batch_size, padded_sequences, tokenizer):\n",
    "        \"\"\"Membuat tf.data.Dataset dengan class weights dan preprocessing yang benar\"\"\"\n",
    "        # Optimasi class weights\n",
    "        class_weights = {0: 1.0, 1: 1.8}  # Sedikit pengurangan dari 2.0\n",
    "        \n",
    "        def generator_fn():\n",
    "            while True:\n",
    "                for query, query_features in query_data:\n",
    "                    # Process query\n",
    "                    clean_query = clean_text(query)\n",
    "                    query_sequence = tokenizer.texts_to_sequences([clean_query])[0]\n",
    "                    \n",
    "                    # Create labels\n",
    "                    labels = create_relevance_labels(df, query_features)\n",
    "                    num_samples = len(labels)\n",
    "                    \n",
    "                    # Generate batches with optimized shuffling\n",
    "                    indices = np.random.permutation(num_samples)\n",
    "                    for start_idx in range(0, num_samples, batch_size):\n",
    "                        end_idx = min(start_idx + batch_size, num_samples)\n",
    "                        batch_indices = indices[start_idx:end_idx]\n",
    "                        \n",
    "                        # Prepare batch data with optimized dtype\n",
    "                        batch_X_text = padded_sequences[batch_indices].astype(np.float32)\n",
    "                        batch_X_numeric = numeric_data[batch_indices].astype(np.float32)\n",
    "                        \n",
    "                        batch_y = np.array(labels)[batch_indices]\n",
    "                        batch_y = np.expand_dims(batch_y, axis=-1)\n",
    "                        \n",
    "                        # Calculate sample weights\n",
    "                        sample_weights = np.array([class_weights[y[0]] for y in batch_y])\n",
    "                        \n",
    "                        yield (\n",
    "                            {\n",
    "                                'text_input': batch_X_text,\n",
    "                                'numeric_input': batch_X_numeric\n",
    "                            },\n",
    "                            batch_y,\n",
    "                            sample_weights\n",
    "                        )\n",
    "        \n",
    "        output_signature = (\n",
    "            {\n",
    "                'text_input': tf.TensorSpec(shape=(None, 300), dtype=tf.float32),\n",
    "                'numeric_input': tf.TensorSpec(shape=(None, 9), dtype=tf.float32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        )\n",
    "        \n",
    "        # Create optimized dataset dengan prefetch\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator_fn,\n",
    "            output_signature=output_signature\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    # Generate training queries\n",
    "    training_queries = prepare_training_queries()\n",
    "    \n",
    "    # Optimasi split ratio\n",
    "    num_queries = len(training_queries)\n",
    "    train_size = int(0.85 * num_queries)  # Sedikit peningkatan dari 0.8\n",
    "    \n",
    "    # Shuffle queries dengan seed untuk reproducibility\n",
    "    np.random.seed(42)  # Tambahan untuk konsistensi\n",
    "    shuffle_idx = np.random.permutation(num_queries)\n",
    "    train_queries = [training_queries[i] for i in shuffle_idx[:train_size]]\n",
    "    val_queries = [training_queries[i] for i in shuffle_idx[train_size:]]\n",
    "    \n",
    "    # Hitung total samples\n",
    "    total_train_samples = sum(len(df) for _, _ in train_queries)\n",
    "    total_val_samples = sum(len(df) for _, _ in val_queries)\n",
    "    \n",
    "    # Buat datasets\n",
    "    train_dataset = create_tf_dataset(\n",
    "        train_queries,\n",
    "        numeric_features,\n",
    "        batch_size,\n",
    "        padded_sequences,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    val_dataset = create_tf_dataset(\n",
    "        val_queries,\n",
    "        numeric_features,\n",
    "        batch_size,\n",
    "        padded_sequences,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = build_model(vocab_size=vocab_size)\n",
    "    \n",
    "    # Validasi output shape\n",
    "    if model.outputs[0].shape[-1] != 1:\n",
    "        raise ValueError(\"Model output shape harus (None, 1)\")\n",
    "    \n",
    "    # Optimasi callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,  # Optimasi dari 10\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'saved_models/best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,  # Optimasi dari 0.2\n",
    "            patience=4,  # Optimasi dari 5\n",
    "            min_lr=0.0001,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model dengan optimasi epochs\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=total_train_samples // batch_size,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=total_val_samples // batch_size,\n",
    "        epochs=5,  # ATUR EPOCH KANG\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyRAG:\n",
    "    def __init__(self, df, model, tokenizer):\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = StemmerFactory().create_stemmer()\n",
    "        self.stopword_factory = StopWordRemoverFactory()  # Create factory\n",
    "        self.stopword = self.stopword_factory.create_stop_word_remover()\n",
    "        \n",
    "        # Tambahkan TF-IDF untuk improved text matching\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=2000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words=self.stopword_factory.get_stop_words()  # Get stop words from factory\n",
    "        )\n",
    "        self.tfidf.fit(df['clean_description'])\n",
    "        self.tfidf_matrix = self.tfidf.transform(df['clean_description'])\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        # Extract numeric requirements dengan error handling\n",
    "        try:\n",
    "            kamar = int(re.findall(r'(\\d+)\\s*kamar tidur', query)[0]) if re.findall(r'(\\d+)\\s*kamar tidur', query) else 0\n",
    "            wc = int(re.findall(r'(\\d+)\\s*kamar mandi', query)[0]) if re.findall(r'(\\d+)\\s*kamar mandi', query) else 0\n",
    "            parkir = int(re.findall(r'(\\d+)\\s*mobil', query)[0]) if re.findall(r'(\\d+)\\s*mobil', query) else 0\n",
    "            \n",
    "            # Extract price range\n",
    "            price_match = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*(?:milyar|miliar|m|M)', query)\n",
    "            max_price = float(price_match[0]) if price_match else float('inf')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extracting numeric values: {e}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Extract location dengan fuzzy matching\n",
    "        locations = ['sleman', 'bantul', 'yogyakarta', 'jogja', 'kulon progo', 'gunung kidul']\n",
    "        query_location = None\n",
    "        max_ratio = 0\n",
    "        \n",
    "        for loc in locations:\n",
    "            ratio = fuzz.partial_ratio(loc, query.lower())\n",
    "            if ratio > max_ratio and ratio > 80:  # Threshold 80%\n",
    "                max_ratio = ratio\n",
    "                query_location = loc\n",
    "        \n",
    "        # Clean and tokenize query dengan improved preprocessing\n",
    "        clean_query = self.preprocess_query(query)\n",
    "        query_sequence = self.tokenizer.texts_to_sequences([clean_query])\n",
    "        padded_query = pad_sequences(query_sequence, maxlen=300, padding='post', truncating='post')\n",
    "        \n",
    "        # Create numeric features dengan additional engineering\n",
    "        numeric_query = np.zeros((1, 9))  # Updated for additional features\n",
    "        if kamar: numeric_query[0][1] = kamar / 10\n",
    "        if wc: numeric_query[0][2] = wc / 10\n",
    "        if parkir: numeric_query[0][3] = parkir / 10\n",
    "        \n",
    "        # Add engineered features\n",
    "        if kamar and wc:\n",
    "            numeric_query[0][7] = (kamar * wc) / 100  # Room-bathroom ratio\n",
    "        \n",
    "        return padded_query, numeric_query, query_location, max_price\n",
    "    \n",
    "    def preprocess_query(self, query):\n",
    "        # Enhanced query preprocessing\n",
    "        query = query.lower()\n",
    "        query = re.sub(r'[^a-zA-Z0-9\\s]', ' ', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "        query = self.stemmer.stem(query)\n",
    "        query = self.stopword.remove(query)\n",
    "        return query\n",
    "    \n",
    "    def enhance_query_understanding(self, query):\n",
    "        \"\"\"\n",
    "        Meningkatkan pemahaman query dengan sinonim dan variasi bahasa\n",
    "        \"\"\"\n",
    "        # Mapping lokasi\n",
    "        location_mapping = {\n",
    "            'jogja': ['yogyakarta', 'jogjakarta', 'yogya'],\n",
    "            'sleman': ['depok', 'condong catur', 'godean'],\n",
    "            'bantul': ['kasihan', 'banguntapan']\n",
    "        }\n",
    "        \n",
    "        # Mapping fasilitas\n",
    "        facility_mapping = {\n",
    "            'kamar': ['ruang tidur', 'bedroom'],\n",
    "            'wc': ['kamar mandi', 'toilet', 'bathroom'],\n",
    "            'parkir': ['garasi', 'carport']\n",
    "        }\n",
    "        \n",
    "        enhanced_query = query.lower()\n",
    "        \n",
    "        # Apply mappings\n",
    "        for main_loc, variants in location_mapping.items():\n",
    "            for var in variants:\n",
    "                if var in enhanced_query:\n",
    "                    enhanced_query = enhanced_query.replace(var, main_loc)\n",
    "        \n",
    "        for main_fac, variants in facility_mapping.items():\n",
    "            for var in variants:\n",
    "                if var in enhanced_query:\n",
    "                    enhanced_query = enhanced_query.replace(var, main_fac)\n",
    "        \n",
    "        return enhanced_query\n",
    "    \n",
    "    def get_recommendations(self, query, top_k=5):\n",
    "        # Process query dengan error handling\n",
    "        result = self.process_query(query)\n",
    "        if result is None:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        padded_query, numeric_query, query_location, max_price = result\n",
    "        \n",
    "        # Filter by location dan price dengan fuzzy matching\n",
    "        filtered_df = self.df.copy()\n",
    "        \n",
    "        if query_location:\n",
    "            location_mask = filtered_df['Lokasi_Clean'].apply(\n",
    "                lambda x: fuzz.partial_ratio(query_location, x.lower()) > 80\n",
    "            )\n",
    "            filtered_df = filtered_df[location_mask]\n",
    "        \n",
    "        # Apply price filter\n",
    "        if max_price != float('inf'):\n",
    "            filtered_df = filtered_df[filtered_df['Harga'] <= (max_price * 1e9)]  # Convert to rupiah\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Calculate text similarity scores\n",
    "        query_tfidf = self.tfidf.transform([self.preprocess_query(query)])\n",
    "        text_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix[filtered_df.index])\n",
    "        \n",
    "        # Get text sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(filtered_df['clean_description'])\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=300, padding='post', truncating='post')\n",
    "        \n",
    "        numeric_features = np.column_stack([\n",
    "            filtered_df[[\n",
    "                'Harga_Normalized', 'Kamar_Normalized', 'WC_Normalized',\n",
    "                'Parkir_Normalized', 'Luas_Tanah_Normalized', 'Luas_Bangunan_Normalized'\n",
    "            ]].values,\n",
    "            filtered_df['Harga_Normalized'] / filtered_df['Luas_Bangunan_Normalized'],\n",
    "            filtered_df['Luas_Bangunan_Normalized'] / filtered_df['Luas_Tanah_Normalized'],\n",
    "            filtered_df['Kamar_Normalized'] * filtered_df['WC_Normalized']\n",
    "        ])\n",
    "        \n",
    "        # Combine model predictions with text similarity\n",
    "        model_predictions = self.model.predict([numeric_features, padded_sequences])\n",
    "        combined_scores = 0.7 * model_predictions.flatten() + 0.3 * text_similarities.flatten()\n",
    "        \n",
    "        # Add scores to DataFrame\n",
    "        filtered_df['relevance_score'] = combined_scores\n",
    "        \n",
    "        # Sort and return recommendations\n",
    "        recommendations = filtered_df.sort_values(\n",
    "            ['relevance_score', 'Harga'],\n",
    "            ascending=[False, True]\n",
    "        ).head(top_k)\n",
    "        \n",
    "        return recommendations[['Judul', 'Lokasi', 'Harga', 'Kamar', 'WC', 'Parkir', \n",
    "                              'Luas_Tanah', 'Luas_Bangunan', 'Property_Link', 'relevance_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_visualization(history, save_path='visualizations/training_history.png'):\n",
    "    \"\"\"\n",
    "    Membuat dan menyimpan visualisasi metrics training model.\n",
    "    \n",
    "    Args:\n",
    "        history: History object dari model.fit() Keras\n",
    "        save_path: Path untuk menyimpan file visualisasi\n",
    "    \"\"\"\n",
    "    # Pastikan direktori exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Set style untuk plot yang lebih baik\n",
    "    plt.style.use('default')  # Menggunakan default style yang pasti ada\n",
    "    \n",
    "    # Buat figure dengan ukuran yang lebih besar dan resolusi tinggi\n",
    "    plt.figure(figsize=(15, 6), dpi=300)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    plt.plot(history['val_loss'], 'r--', label='Validation Loss', linewidth=2)\n",
    "    plt.title('Model Loss', fontsize=12, pad=15)\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.ylabel('Loss', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(history['val_accuracy'], 'r--', label='Validation Accuracy', linewidth=2)\n",
    "    plt.title('Model Accuracy', fontsize=12, pad=15)\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.ylabel('Accuracy', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # Atur layout dan margin\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    # Debug print untuk memastikan data ada\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"History keys: {list(history.keys())}\")\n",
    "    print(f\"Number of epochs: {len(history['loss'])}\")\n",
    "    \n",
    "    # Simpan plot dengan kualitas tinggi\n",
    "    try:\n",
    "        plt.savefig(save_path, \n",
    "                   format='png',\n",
    "                   bbox_inches='tight',\n",
    "                   pad_inches=0.2,\n",
    "                   dpi=300)\n",
    "        print(f\"\\nVisualisasi berhasil disimpan di: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menyimpan visualisasi: {str(e)}\")\n",
    "    finally:\n",
    "        # Pastikan untuk menutup figure untuk menghemat memori\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Create necessary directories\n",
    "        create_directories()\n",
    "        \n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        # Load and preprocess data\n",
    "        df = load_and_preprocess_data('processed_house_data.csv')\n",
    "        \n",
    "        print(\"Preparing features...\")\n",
    "        # Prepare features\n",
    "        padded_sequences, numeric_features, tokenizer = prepare_features(df)\n",
    "        \n",
    "        print(\"Starting model training...\")\n",
    "        # Train model\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        model, history = train_improved_model_batched(df, padded_sequences, numeric_features, tokenizer)\n",
    "        \n",
    "        # Buat visualisasi menggunakan fungsi yang telah diperbaiki\n",
    "        print(\"\\nMembuat visualisasi training history...\")\n",
    "        create_training_visualization(history.history)\n",
    "        \n",
    "        # Print metrics training\n",
    "        print(\"\\nHasil Training Model:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric in history.history.keys():\n",
    "            final_value = history.history[metric][-1]\n",
    "            print(f\"{metric}: {final_value:.4f}\")\n",
    "        \n",
    "        # Initialize RAG system\n",
    "        print(\"\\nInitializing RAG system...\")\n",
    "        rag_system = PropertyRAG(df, model, tokenizer)\n",
    "        \n",
    "        return rag_system, history.history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error terjadi selama eksekusi: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Preparing features...\n",
      "Starting model training...\n",
      "Epoch 1/5\n",
      "\u001b[1m467/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6994 - auc_9: 0.6256 - f1_score: 0.2722 - loss: 0.7121 - precision_9: 0.2420 - recall_9: 0.3991\n",
      "Epoch 1: val_loss improved from inf to 0.57952, saving model to saved_models/best_model.keras\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.6997 - auc_9: 0.6257 - f1_score: 0.2722 - loss: 0.7117 - precision_9: 0.2421 - recall_9: 0.3986 - val_accuracy: 0.8464 - val_auc_9: 0.7184 - val_f1_score: 0.2715 - val_loss: 0.5795 - val_precision_9: 0.6707 - val_recall_9: 0.0436 - learning_rate: 9.3858e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8246 - auc_9: 0.7001 - f1_score: 0.2844 - loss: 0.6101 - precision_9: 0.4269 - recall_9: 0.1657\n",
      "Epoch 2: val_loss did not improve from 0.57952\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8246 - auc_9: 0.7001 - f1_score: 0.2845 - loss: 0.6102 - precision_9: 0.4269 - recall_9: 0.1658 - val_accuracy: 0.8060 - val_auc_9: 0.7077 - val_f1_score: 0.3009 - val_loss: 0.6115 - val_precision_9: 0.4171 - val_recall_9: 0.2403 - learning_rate: 8.8094e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m467/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8378 - auc_9: 0.6860 - f1_score: 0.2486 - loss: 0.5642 - precision_9: 0.3506 - recall_9: 0.1601\n",
      "Epoch 3: val_loss did not improve from 0.57952\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8378 - auc_9: 0.6860 - f1_score: 0.2487 - loss: 0.5643 - precision_9: 0.3507 - recall_9: 0.1601 - val_accuracy: 0.8323 - val_auc_9: 0.6968 - val_f1_score: 0.2858 - val_loss: 0.6052 - val_precision_9: 0.4837 - val_recall_9: 0.0888 - learning_rate: 8.2684e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m466/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8472 - auc_9: 0.7239 - f1_score: 0.2466 - loss: 0.5345 - precision_9: 0.3968 - recall_9: 0.1616\n",
      "Epoch 4: val_loss improved from 0.57952 to 0.56686, saving model to saved_models/best_model.keras\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8472 - auc_9: 0.7238 - f1_score: 0.2467 - loss: 0.5347 - precision_9: 0.3970 - recall_9: 0.1617 - val_accuracy: 0.8330 - val_auc_9: 0.7421 - val_f1_score: 0.2840 - val_loss: 0.5669 - val_precision_9: 0.4878 - val_recall_9: 0.1805 - learning_rate: 7.7606e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m466/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7986 - auc_9: 0.6855 - f1_score: 0.3219 - loss: 0.6660 - precision_9: 0.4360 - recall_9: 0.1585\n",
      "Epoch 5: val_loss did not improve from 0.56686\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7987 - auc_9: 0.6855 - f1_score: 0.3217 - loss: 0.6656 - precision_9: 0.4359 - recall_9: 0.1585 - val_accuracy: 0.8188 - val_auc_9: 0.6971 - val_f1_score: 0.3111 - val_loss: 0.6545 - val_precision_9: 0.5732 - val_recall_9: 0.0635 - learning_rate: 7.2839e-04\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Membuat visualisasi training history...\n",
      "\n",
      "Debug information:\n",
      "History keys: ['accuracy', 'auc_9', 'f1_score', 'loss', 'precision_9', 'recall_9', 'val_accuracy', 'val_auc_9', 'val_f1_score', 'val_loss', 'val_precision_9', 'val_recall_9', 'learning_rate']\n",
      "Number of epochs: 5\n",
      "\n",
      "Visualisasi berhasil disimpan di: visualizations/training_history.png\n",
      "\n",
      "Hasil Training Model:\n",
      "--------------------------------------------------\n",
      "accuracy: 0.8220\n",
      "auc_9: 0.6954\n",
      "f1_score: 0.2877\n",
      "loss: 0.6067\n",
      "precision_9: 0.4212\n",
      "recall_9: 0.1585\n",
      "val_accuracy: 0.8188\n",
      "val_auc_9: 0.6971\n",
      "val_f1_score: 0.3111\n",
      "val_loss: 0.6545\n",
      "val_precision_9: 0.5732\n",
      "val_recall_9: 0.0635\n",
      "learning_rate: 0.0007\n",
      "\n",
      "Initializing RAG system...\n",
      "\n",
      "==================================================\n",
      "Query: saya ingin rumah di sleman dengan 3 kamar tidur, 2 kamar mandi dan slot parkir 2 mobil\n",
      "==================================================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\galan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: {'text_input': 'text_input', 'numeric_input': 'numeric_input'}. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rekomendasi Properti:\n",
      "\n",
      "Properti 68:\n",
      "Judul: Rumah Cantik Estetik Limasan Di Jalan Sidomoyo Km 1 Godean Sleman\n",
      "Lokasi: Sleman, Yogyakarta\n",
      "Harga: Rp 785,000,000.0\n",
      "Kamar Tidur: 3.0\n",
      "Kamar Mandi: 2.0\n",
      "Parkir: 2.0\n",
      "Luas Tanah: 110.0 m²\n",
      "Luas Bangunan: 75.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 83:\n",
      "Judul: Rumah Baru Mewah Fresh Limasan Modern Dalam Mini Cluster Madinah Residence Di Jalan Kaliurang Km 13 Dekat Sma N Ngaglik Sleman\n",
      "Lokasi: Ngaglik, Sleman\n",
      "Harga: Rp 655,000,000.0\n",
      "Kamar Tidur: 3.0\n",
      "Kamar Mandi: 2.0\n",
      "Parkir: 2.0\n",
      "Luas Tanah: 104.0 m²\n",
      "Luas Bangunan: 80.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 8:\n",
      "Judul: Rumah Baru 3 Unit Mewah Limasan Harga Murah Area Perumahan Di Jalan Kaliurang Km 13 Dekat Sma N 2 Ngaglik Dan Spbu Mindi Sleman\n",
      "Lokasi: Ngemplak, Sleman\n",
      "Harga: Rp 650,000,000.0\n",
      "Kamar Tidur: 3.0\n",
      "Kamar Mandi: 2.0\n",
      "Parkir: 2.0\n",
      "Luas Tanah: 90.0 m²\n",
      "Luas Bangunan: 65.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 67:\n",
      "Judul: Rumah Baru SHM Siap Huni, 8 Menit Ke Pemda Sleman Di Mlati Sleman\n",
      "Lokasi: Mlati, Sleman\n",
      "Harga: Rp 769,000,000.0\n",
      "Kamar Tidur: 3.0\n",
      "Kamar Mandi: 2.0\n",
      "Parkir: 2.0\n",
      "Luas Tanah: 130.0 m²\n",
      "Luas Bangunan: 60.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 11:\n",
      "Judul: Grand Launching Minimalis Kekinian di Sleman Utara Dekat Uii\n",
      "Lokasi: Sleman, Yogyakarta\n",
      "Harga: Rp 768,000,000.0\n",
      "Kamar Tidur: 2.0\n",
      "Kamar Mandi: 2.0\n",
      "Parkir: 1.0\n",
      "Luas Tanah: 96.0 m²\n",
      "Luas Bangunan: 60.0 m²\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "Query: cari rumah di bantul dengan budget 500 juta dengan 2 kamar tidur\n",
      "==================================================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\galan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: {'text_input': 'text_input', 'numeric_input': 'numeric_input'}. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rekomendasi Properti:\n",
      "\n",
      "Properti 53:\n",
      "Judul: Rumah Cantik Scandinavian Dengan Mezanin Kekinian Dalam Perumahan Puri Ismail Dekat Giwangan Banguntapan Bantul\n",
      "Lokasi: Banguntapan, Bantul\n",
      "Harga: Rp 650,000,000.0\n",
      "Kamar Tidur: 2.0\n",
      "Kamar Mandi: 1.0\n",
      "Parkir: 1.0\n",
      "Luas Tanah: 98.0 m²\n",
      "Luas Bangunan: 60.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 50:\n",
      "Judul: Promo Bulan November 375 Juta Saja! Rumah Cantik Mewah - Harga Murah Dalam Perumahan Fasco Village Bangunjiwo Kasihan Bantul\n",
      "Lokasi: Kasihan, Bantul\n",
      "Harga: Rp 350,000,000.0\n",
      "Kamar Tidur: 2.0\n",
      "Kamar Mandi: 1.0\n",
      "Parkir: 1.0\n",
      "Luas Tanah: 80.0 m²\n",
      "Luas Bangunan: 36.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 85:\n",
      "Judul: RUMAH BARU SIAP HUNI CANTIK MINIMALIS HARGA EKONOMIS HOOK DALAM CLUSTER MUTIARA SEWON JALAN IMOGIRI BARAT KM 6\n",
      "Lokasi: Sewon, Bantul\n",
      "Harga: Rp 495,000,000.0\n",
      "Kamar Tidur: 2.0\n",
      "Kamar Mandi: 1.0\n",
      "Parkir: 1.0\n",
      "Luas Tanah: 85.0 m²\n",
      "Luas Bangunan: 50.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 6:\n",
      "Judul: Minimalis Modern Cocok Untuk Gaya Hidup Praktis Dan Stylish\n",
      "Lokasi: Sedayu, Bantul\n",
      "Harga: Rp 710,000,000.0\n",
      "Kamar Tidur: 2.0\n",
      "Kamar Mandi: 1.0\n",
      "Parkir: 1.0\n",
      "Luas Tanah: 72.0 m²\n",
      "Luas Bangunan: 50.0 m²\n",
      "------------------------------\n",
      "\n",
      "Properti 87:\n",
      "Judul: Dijual Villa Murah Mangku Jalan Utama View Sungai Sawah Pegunungan\n",
      "Lokasi: Banguntapan, Bantul\n",
      "Harga: Rp 345,000,000,000.0\n",
      "Kamar Tidur: 5.0\n",
      "Kamar Mandi: 6.0\n",
      "Parkir: 5.0\n",
      "Luas Tanah: 735.0 m²\n",
      "Luas Bangunan: 300.0 m²\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "Query: rekomendasi rumah di kota jogja dengan luas tanah minimal 100m2\n",
      "==================================================\n",
      "\n",
      "Rekomendasi Properti:\n",
      "\n",
      "Model Performance Metrics:\n",
      "--------------------------------------------------\n",
      "accuracy: [0.7811573147773743, 0.8047488331794739, 0.8325048089027405, 0.8377158641815186, 0.8220390677452087]\n",
      "auc_9: [0.6618438363075256, 0.6921138763427734, 0.6976343393325806, 0.7176905870437622, 0.6953580975532532]\n",
      "f1_score: [0.2568838596343994, 0.3124275207519531, 0.26341792941093445, 0.26551946997642517, 0.28766635060310364]\n",
      "loss: [0.6192790865898132, 0.6508249044418335, 0.574645459651947, 0.5664722323417664, 0.6066864728927612]\n",
      "precision_9: [0.27147239446640015, 0.4360881447792053, 0.3786407709121704, 0.4228801131248474, 0.42117971181869507]\n",
      "recall_9: [0.28806036710739136, 0.18643268942832947, 0.1625700742006302, 0.16479134559631348, 0.15846852958202362]\n",
      "val_accuracy: [0.8464410305023193, 0.8059980273246765, 0.8322548270225525, 0.8330014944076538, 0.8188153505325317]\n",
      "val_auc_9: [0.718390703201294, 0.7076904773712158, 0.6968060731887817, 0.7420839667320251, 0.6970796585083008]\n",
      "val_f1_score: [0.27145618200302124, 0.3008774220943451, 0.2858361601829529, 0.2840059697628021, 0.3110550343990326]\n",
      "val_loss: [0.579521894454956, 0.6115493178367615, 0.6052229404449463, 0.5668624043464661, 0.6545233726501465]\n",
      "val_precision_9: [0.6707317233085632, 0.41707316040992737, 0.48373982310295105, 0.4878048896789551, 0.5731707215309143]\n",
      "val_recall_9: [0.04358161613345146, 0.2403373122215271, 0.08880597352981567, 0.18045112490653992, 0.06351350992918015]\n",
      "learning_rate: [0.0009385845623910427, 0.0008809409337118268, 0.000826837494969368, 0.0007760568987578154, 0.0007283949526026845]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run main function\n",
    "        rag_system, metrics = main()\n",
    "       \n",
    "        # Example queries\n",
    "        test_queries = [\n",
    "            \"saya ingin rumah di sleman dengan 3 kamar tidur, 2 kamar mandi dan slot parkir 2 mobil\",\n",
    "            \"cari rumah di bantul dengan budget 500 juta dengan 2 kamar tidur\",\n",
    "            \"rekomendasi rumah di kota jogja dengan luas tanah minimal 100m2\"\n",
    "        ]\n",
    "       \n",
    "        for query in test_queries:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Query: {query}\")\n",
    "            print(\"=\"*50)\n",
    "           \n",
    "            recommendations = rag_system.get_recommendations(query)\n",
    "           \n",
    "            print(\"\\nRekomendasi Properti:\")\n",
    "            for idx, row in recommendations.iterrows():\n",
    "                print(f\"\\nProperti {idx + 1}:\")\n",
    "                print(f\"Judul: {row['Judul']}\")\n",
    "                print(f\"Lokasi: {row['Lokasi']}\")\n",
    "                print(f\"Harga: Rp {row['Harga']:,}\")\n",
    "                print(f\"Kamar Tidur: {row['Kamar']}\")\n",
    "                print(f\"Kamar Mandi: {row['WC']}\")\n",
    "                print(f\"Parkir: {row['Parkir']}\")\n",
    "                print(f\"Luas Tanah: {row['Luas_Tanah']} m²\")\n",
    "                print(f\"Luas Bangunan: {row['Luas_Bangunan']} m²\")\n",
    "                print(\"-\"*30)\n",
    "               \n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        print(\"-\"*50)\n",
    "        for metric, value in metrics.items():\n",
    "            # Check if value is a number before formatting\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        print(\"Please ensure all directories and files exist and are accessible.\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
